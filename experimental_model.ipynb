{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Plant Disease Detection Plan Path\n",
    "\n",
    "/path/to/dataset/\n",
    "│\n",
    "├── train/\n",
    "│   ├── wortel/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── wortel_disease1/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── tomat/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── tomat_disease1/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "│\n",
    "├── val/\n",
    "│   ├── wortel/\n",
    "│   ├── wortel_disease1/\n",
    "│   ├── tomat/\n",
    "│   ├── tomat_disease1/\n",
    "│   └── ...\n",
    "│\n",
    "└── test/\n",
    "    ├── wortel/\n",
    "    ├── wortel_disease1/\n",
    "    ├── tomat/\n",
    "    ├── tomat_disease1/\n",
    "    └── ...\n",
    "\n",
    "\n",
    "80% - Train\n",
    "10% - Validation\n",
    "10% - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    ")\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = r\"/splitedDataset\"\n",
    "TRAIN_DIR = r\"/splitedDataset/train_ds\"\n",
    "VAL_DIR = r\"/splitedDataset/val_ds\"\n",
    "TEST_DIR = r\"/splitedDataset/test_ds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_PLANT = [\"wortel\", \"tomat\", \"timun\", \"sawi\"]\n",
    "CLASSES_DISEASE = [\"healthy\", \"leaf_blight\", \"powdery_mildew\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipeline(train_dir, val_dir, test_dir):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomTranslation(0.2, 0.2),\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "        layers.GaussianNoise(0.1),\n",
    "    ])\n",
    "\n",
    "    # Preprocessing layer\n",
    "    preprocessing = tf.keras.Sequential([\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Normalization()\n",
    "    ])\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        seed=42,\n",
    "        batch_size=32,\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(256, 256),\n",
    "    )\n",
    "\n",
    "    validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        VAL_DIR,\n",
    "        seed=42,\n",
    "        batch_size=32,\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(256, 256),\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TEST_DIR,\n",
    "        seed=42,\n",
    "        batch_size=32,\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=(256, 256),\n",
    "    )\n",
    "\n",
    "    # Apply augmentation only to training data\n",
    "    train_ds = train_ds.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing to all datasets\n",
    "    train_ds = train_ds.map(\n",
    "        lambda x, y: (preprocessing(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    val_ds = val_ds.map(\n",
    "        lambda x, y: (preprocessing(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    test_ds = test_ds.map(\n",
    "        lambda x, y: (preprocessing(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # Prefetch for performance\n",
    "    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = create_data_pipeline(\n",
    "    train_dir=TRAIN_DIR, val_dir=VAL_DIR, test_dir=TEST_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_plants, num_diseases):\n",
    "    base_model = keras.applications.EfficientNetV2B0(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "\n",
    "    # Progressive learning rate unfreeze\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create model with attention mechanism\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs)\n",
    "\n",
    "    # Add attention mechanism\n",
    "    attention = layers.GlobalAveragePooling2D()(x)\n",
    "    attention = layers.Dense(1024, activation=\"relu\")(attention)\n",
    "    attention = layers.Dense(x.shape[-1], activation=\"sigmoid\")(attention)\n",
    "    attention = layers.Reshape((1, 1, x.shape[-1]))(attention)\n",
    "    x = layers.Multiply()([x, attention])\n",
    "\n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Plant type branch\n",
    "    plant_branch = layers.Dense(512, activation=\"relu\")(x)\n",
    "    plant_branch = layers.BatchNormalization()(plant_branch)\n",
    "    plant_branch = layers.Dropout(0.3)(plant_branch)\n",
    "    plant_output = layers.Dense(num_plants, activation=\"softmax\", name=\"plant_type\")(plant_branch)\n",
    "\n",
    "    # Disease type branch\n",
    "    disease_branch = layers.Dense(512, activation=\"relu\")(x)\n",
    "    disease_branch = layers.BatchNormalization()(disease_branch)\n",
    "    disease_branch = layers.Dropout(0.3)(disease_branch)\n",
    "    disease_output = layers.Dense(num_diseases, activation=\"softmax\", name=\"disease_type\")(disease_branch)\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=[plant_output, disease_output])\n",
    "\n",
    "    # Compile with weighted losses and learning rate scheduler\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps, decay_rate\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            \"plant_type\": \"categorical_crossentropy\",\n",
    "            \"disease_type\": \"categorical_crossentropy\"\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"plant_type\": 1.0,\n",
    "            \"disease_type\": 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            \"plant_type\": [\"accuracy\", tf.keras.metrics.AUC()],\n",
    "            \"disease_type\": [\"accuracy\", tf.keras.metrics.AUC()]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "model = create_model(num_plants=len(CLASSES_PLANT), num_diseases=len(CLASSES_DISEASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, epochs=EPOCHS):\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.h5\", monitor=\"val_loss\", save_best_only=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "history, trained_model = train_model(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    epochs=50,  # Sesuaikan dengan kebutuhan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "print(\"Evaluating model...\")\n",
    "test_results = trained_model.evaluate(test_ds)\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Plant Type Loss: {test_results[1]:.4f}\")\n",
    "print(f\"Plant Type Accuracy: {test_results[3]:.4f}\")\n",
    "print(f\"Plant Type AUC: {test_results[4]:.4f}\")\n",
    "print(f\"Disease Type Loss: {test_results[2]:.4f}\")\n",
    "print(f\"Disease Type Accuracy: {test_results[5]:.4f}\")\n",
    "print(f\"Disease Type AUC: {test_results[6]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training History\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot akurasi\n",
    "    ax1.plot(history.history[\"plant_type_accuracy\"], label=\"Plant Training Accuracy\")\n",
    "    ax1.plot(\n",
    "        history.history[\"val_plant_type_accuracy\"], label=\"Plant Validation Accuracy\"\n",
    "    )\n",
    "    ax1.plot(\n",
    "        history.history[\"disease_type_accuracy\"], label=\"Disease Training Accuracy\"\n",
    "    )\n",
    "    ax1.plot(\n",
    "        history.history[\"val_disease_type_accuracy\"],\n",
    "        label=\"Disease Validation Accuracy\",\n",
    "    )\n",
    "    ax1.set_title(\"Model Accuracy\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history[\"plant_type_loss\"], label=\"Plant Training Loss\")\n",
    "    ax2.plot(history.history[\"val_plant_type_loss\"], label=\"Plant Validation Loss\")\n",
    "    ax2.plot(history.history[\"disease_type_loss\"], label=\"Disease Training Loss\")\n",
    "    ax2.plot(history.history[\"val_disease_type_loss\"], label=\"Disease Validation Loss\")\n",
    "    ax2.set_title(\"Model Loss\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "eval_results = model.evaluate(validation_data)\n",
    "print(f\"Plant Type Accuracy: {eval_results[1]:.2f}\")\n",
    "print(f\"Disease Type Accuracy: {eval_results[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "print(\"Saving model...\")\n",
    "trained_model.save(\"plant_disease_model_final.h5\")\n",
    "print(\"Model saved as 'plant_disease_model_final.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # Predict\n",
    "    plant_pred, disease_pred = model.predict(img_array)\n",
    "\n",
    "    # Choose Highest Confidence\n",
    "    plant_class = CLASSES_PLANT[tf.argmax(plant_pred[0])]\n",
    "    disease_class = CLASSES_DISEASE[tf.argmax(disease_pred[0])]\n",
    "\n",
    "    return {\n",
    "        \"plant\": plant_class,\n",
    "        \"plant_confidence\": float(tf.reduce_max(plant_pred[0])),\n",
    "        \"disease\": disease_class,\n",
    "        \"disease_confidence\": float(tf.reduce_max(disease_pred[0])),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_image(trained_model, 'path/to/image.jpg')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, average=\"macro\"):\n",
    "    return f1_score(y_true, y_pred, average=average)\n",
    "\n",
    "\n",
    "# Calculate F1 score\n",
    "y_true = np.argmax(test_ds.labels, axis=1)\n",
    "y_pred = np.argmax(model.predict(test_ds), axis=1)\n",
    "f1_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "# Print the F1 score\n",
    "print(f\"F1 score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
